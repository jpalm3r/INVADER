{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#58a4b0> Project B: Social Data Analysis and Visualization (02806) </font>\n",
    "## <font color=grey> Spring 2020 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "The data set consists in data I scraped from the API, [PushShift](https://pushshift.io/). There are several parameters that can be set when using the API. I decided to download the most relevant posts on a daily basis according to a reddit score of relevance. This score depends on the number of upvotes and comments that a post had at the time the API stored it.\n",
    "\n",
    "The features that my data has are:\n",
    "\n",
    " - Time of the post (in UTC)\n",
    " - Post title\n",
    " - Post score\n",
    " - subreddit community\n",
    "\n",
    "I downloaded data from the first of January 2020 to April the 13th; the last day being the day I downloaded data for the first time.\n",
    "\n",
    "The main point of my visualization was to track the amount of COVID-19 related content that was being posted since the beginning of the year, and how, during those weeks, the media has been so full of this kind of content, that made almost every conversation gravitate towards that. The features I described were sufficient to show it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic stats\n",
    "\n",
    "\n",
    "The data I worked with is very straightforward. There was no need for data cleaning, since I was specifying the data I wanted before the download. All my figures are based on daily data; this data was downloaded directly making a query to the API specifying exact dates. For this reason, I did not perform any additional data manipulation.\n",
    "\n",
    "### Downloading\n",
    "\n",
    "I put together a [script](https://github.com/jpalm3r/INVADER/blob/master/download_data.py) to download the data and saved in different files. As it can be seen on the script, I made a query to the API to download daily data starting 01/01/2020. Then, for every day, I downloaded two data frames: one containing the post title and the other the rest of the information.\n",
    "\n",
    "I made two different kinds of queries to the API: __general content__ and __filtering by community__. The reason for the first one was that I wanted to see the change in the landscape of the Reddit main page. This ended being the first plot on my webpage. On the other hand,  I was interested in finding the COVID-19 content at the specific subreddit level. So I selected some of the major communities that could be affected by COVID-19 and downloaded the data specifying the community.\n",
    "\n",
    "\n",
    "All my queries were sorted in descending order by Reddit score; i.e. if I requested _N_ posts, they were the _N_ posts with a higher score at the time they were stored by PushShift. The API request process was unstable, specially when downloading content from the main page. I made queries of the 400 daily top posts but often I was unable to get all the content I requested, and ended with slightly less posts. However, in general I always got almost 400 daily posts which were used to compute the daily distribution.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jpalm3r/INVADER/master/num_of_posts.png\" alt=\"Drawing\" style=\"width: 800px;\">\n",
    "\n",
    "Below, an sample of how the raw data looked for an arbitrary day in the posts I downloaded from the main page, where the column names are in `column_names`. Notice that the variables of `time_local`, `id` and `ncomments` were not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['time_utc', 'epoch_utc', 'time_local', 'epoch_local', 'id', 'score', 'subreddit', 'ncomments']\n",
    "\n",
    "df = '''\\\n",
    "2020-04-11 20:41:29\t1586630489.0\t2020-04-12 04:41:29\t1586659289.0\tfzdovx\t78719\tCoronavirus\t3030\n",
    "2020-04-11 23:22:30\t1586640150.0\t2020-04-12 07:22:30\t1586668950.0\tfzjfdy\t74194\tBlackPeopleTwitter\t781\n",
    "2020-04-12 01:44:31\t1586648671.0\t2020-04-12 09:44:31\t1586677471.0\tfzlr68\t75694\ttodayilearned\t1681\n",
    "2020-04-12 01:38:54\t1586648334.0\t2020-04-12 09:38:54\t1586677134.0\tfzlnuc\t78347\tMurderedByWords\t935\n",
    "2020-04-12 01:49:36\t1586648976.0\t2020-04-12 09:49:36\t1586677776.0\tfzlu7z\t68127\tpics\t994\n",
    "2020-04-11 23:11:07\t1586639467.0\t2020-04-12 07:11:07\t1586668267.0\tfzj8ph\t62637\twholesomememes\t182\n",
    "2020-04-11 22:03:00\t1586635380.0\t2020-04-12 06:03:00\t1586664180.0\tfzh78y\t64849\tpolitics\t5770\n",
    "2020-04-11 20:55:05\t1586631305.0\t2020-04-12 04:55:05\t1586660105.0\tfze9g5\t65043\twholesomememes\t303\n",
    "2020-04-11 21:51:43\t1586634703.0\t2020-04-12 05:51:43\t1586663503.0\tfzgpmb\t63483\taww\t554\n",
    "2020-04-11 22:30:01\t1586637001.0\t2020-04-12 06:30:01\t1586665801.0\tfzi9x6\t68043\tWellthatsucks\t2660\n",
    "2020-04-11 23:38:30\t1586641110.0\t2020-04-12 07:38:30\t1586669910.0\tfzjono\t62826\tgaming\t296\n",
    "2020-04-12 00:07:34\t1586642854.0\t2020-04-12 08:07:34\t1586671654.0\tfzk5rm\t62515\tinterestingasfuck\t711\n",
    "2020-04-11 23:54:53\t1586642093.0\t2020-04-12 07:54:53\t1586670893.0\tfzjydb\t57950\tmemes\t638\n",
    "2020-04-11 19:51:45\t1586627505.0\t2020-04-12 03:51:45\t1586656305.0\tfzbjr9\t92124\tgifs\t1432\n",
    "2020-04-12 00:38:02\t1586644682.0\t2020-04-12 08:38:02\t1586673482.0\tfzkncp\t71502\tnottheonion\t2720\n",
    "'''\n",
    "\n",
    "titles = '''\\\n",
    "For the first time in US history, every state is under a disaster declaration simultaneously\n",
    "Snoozing ain’t the same anymore\n",
    "TIL Classical Greece wasn't filled with pure white marble everything. The statues and ruins we see today were actually painted in all kinds of vivid colors that were just stripped bare back to their original white marble by time.\n",
    "Rip cooper’s dog, and Eric\n",
    "Just painted for 4.5 hours!\n",
    "Do your best.\n",
    "Trump reportedly said he would reject a bailout package if it included aid to keep the US Postal Service functioning\n",
    "Wholesome Couple.\n",
    "Baby black panther\n",
    "Fake ThermoScan from china that will never exceed 37C\n",
    "Prom Night\n",
    "This 3D-printed \"digital\" sundial accurately projects the time onto the ground in a recognisable digital clock style!\n",
    "Essential item for the frontline\n",
    "How To Make Infinite Loop Using Watering Cans GIF\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Requesting data from the API was a slow process. It could take around 10 hours to download 400 daily posts from the first of January to mid April. Also, since it crashed often, the procces was difficult to make fully automatic.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Storing\n",
    " \n",
    "The script was designed to store the files in different folders,depending on the community I requested. The structure is represented in the following diagram:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jpalm3r/INVADER/master/folder_path.png\" alt=\"Drawing\" style=\"width: 540px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading\n",
    "\n",
    "Once the data was downloaded, it was completely handled by a [jupyter notebook](https://github.com/jpalm3r/INVADER/blob/master/Project_B.ipynb). There, the data was read according to the folder configuration I described before.\n",
    "\n",
    "### Exploring\n",
    "\n",
    "The objective of this project was clear from the beginning, as well as the necessary resources to create the visualization. Since I was selecting the relevant content before the download, and storing it conveniently, it was not necessary to spend time understanding the data set. On the other hand, I needed to spend time understanding the different communities that appear frequently in Reddit main page to be able to create the final visualizations, as I explain in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "One of the main take-home messages after this project is the amount of communities that exist, are hugely popular, and I had never heard of them. I use reddit on a daily basis and when it was time to classify the content in the main page I realized that I needed a very coarse categorization due to the numerous different categories that appear in the main page.\n",
    "\n",
    "The data has 40997 rows, accounting for ~400 posts for ~100 days. In this data, there are 646 different communities. Every top 400 daily posts have around 40-50 different communities as can be seen below: \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jpalm3r/INVADER/master/num_of_subreddits_byday.png\" alt=\"Drawing\" style=\"width: 800px;\">\n",
    "\n",
    "I started basing my categories on this [classification](https://www.reddit.com/r/ListOfSubreddits/wiki/listofsubreddits). This classification was done manually, some of the categories that I tried can be seen [here](https://github.com/jpalm3r/INVADER/blob/master/lists.zip). However, many communities that are not even part of these lists kept appearing; so finally, I decided to define the broad classes that can be seen in the website.\n",
    "\n",
    "A key part of my visualization is the classification method to identify COVID-related posts at subreddit level. In order to do that, I used a simple function that identified posts talking about the pandemic (_CovidRelated(...)_ in [here](https://github.com/jpalm3r/INVADER/blob/master/Project_B.ipynb)). This method misses identifying content that addreses coronavirus without using those terms; however, the results were satisfactory to make the point I had in mind at the beginning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre\n",
    "\n",
    "My main inspirations to tell the story were this [piece](https://www.washingtonpost.com/graphics/2020/world/corona-simulator/) about different spreading outcomes and the [visualization](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) we used to get introduced to decision trees. Both have a __magazine style__, where the story is __linear__ and the text and figures are tightly related. In addition, they incorporate annotations and complementary plots, to make the story more rich.\n",
    "\n",
    "My short story is also linear and __author-driven__, but it is centered around the timeline of tileplots of <span style=\"background-color:#DBB04A\">r/worldnews</span>. There, the reader can spend most of the time hovering over the different days and discovering different news narrating the spread of the pandemic.\n",
    "\n",
    "In order to tell the story I wanted an introduction and an ending. The introduction is meant to give an overview of the problem, introducing the presence of COVID content in reddit main page. First, I make an introductory text, that leads to the first plot. This plot has __annotations__ that help following the timeline. Then I present the problem at the different communities wich leads to the main visualization. After the main plot, I present the reaction to the pandemic in communities were the users create their own content. This plot has very similar structure than the first one but it is used as a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "\n",
    "I have always thought about this visualization project in terms of space. COVID-19 has pushed many of us to a claustrophobic situation, physically and mentally speaking. For this reason I wanted to compare areas. The main concept idea was creating an animation, such as this one:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jpalm3r/INVADER/master/animation.gif\" alt=\"Drawing\" style=\"width: 200px;\">\n",
    "\n",
    "Then, I decided to incroporate the hovering part to be able to read the titles. Which led to the main visualization in my page, and made me exclude the animation since it was redundant.\n",
    "I decided to use as few time information as possible to emphasize the stagnation and repetitiveness that COVID-19 has imposed.\n",
    "\n",
    "This visualization shows a random sample of 100 posts for each day. These 100 posts are taken randomly from a sample of ~400 posts. I decided to use 100 posts because of I liked the square representation, as a unit of daily content. This format also allowed the hovering over the tiles for reading the different post titles.<br>\n",
    "It can be seen in this visualization that the community of <span style=\"background-color:#DBB04A\">r/worldnews</span> reached a point where most of its content is colore red <span style=\"background-color:#F93822\">__</span>, which highlights the infectious nature of COVID-19 content.\n",
    "\n",
    "Before the main visualization, I introduce the reader to this idea of taking <span style=\"background-color:#F93822\">r/coronavirus</span> taking over showing the evolution of the distribution of content in Reddit main page. I decided to use stacked bar charts because they also show areas which matched the motivation of the project. For every day, I have a bar which still holds the notion of _unit of daily content_. Each bar has a distribution of content; when put together in the timeline, they show clearly the evolution of each category. This is also one of the main reasons I used very coarse classification: to be able to use the stacked bar format, without making a figure that would be too busy.\n",
    "\n",
    "In the final plot I added a simple smoothed curve representing the trend of the humor content in the last weeks, since I wanted to finsih with a positive note. The simple line helps conveying the idea that, overall, the humor content has not been decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "I think the story might be confusing to a reader is not familiar with reddit and its structure. The parts where I talk about communities aggregated into categories could be explained more carefully. Additionally, the categorization is explained fast and it might sound somehow arbitrary. More time could be spent on grounding more these categories.<br>I would like to do a more sophisticated text analysis using machine learning (ML), because I think that my simple classification using terms is too coarse and I miss numerous COVID-19 related plots. Using a ML text classifier would allow a deeper and more reliable analysis.\n",
    "\n",
    "The results only scratch the surface and there are sections that could be interesting to dig deeper in. Such as the actual correlation between the peaks and the events, or seeing the relationship between the real infection trend and the amount of COVID content being posted in different communities. However, all these missing aspects could become pitfalls if the time is tight, since it can be dangerous to have too many ambitions in open-ended projects such as this one. \n",
    "\n",
    "After all this work, I am satisfied with the webpage. The plots represent the idea I had at the beginning of this project, the style is appealing, and the figures are easy to understand. The story works in the webpage format and the interactive hovering part is a real upgrade. Thanks to this, I was able to have a better idea of the Bokeh library and its capabilities. Finally, I had the opportunity to put together a webpage for the first time, which is a very interesting skill that I take home."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
